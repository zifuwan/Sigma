<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation">
  <meta property="og:title" content="Sigma"/>
  <meta property="og:description" content="Siamese Mamba Network for Multi-Modal Semantic Segmentation"/>
  <meta property="og:url" content="https://zifuwan.github.io/Sigma/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation</title>
  <link rel="icon" type="image/x-icon" href="static/images/logo.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://zifuwan.github.io/" target="_blank">Zifu Wan</a>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=MfbIbuEAAAAJ&hl=zh-CN" target="_blank">Pingping Zhang</a>,</span>
                    <span class="author-block">
                      <a href="https://924973292.github.io/" target="_blank">Yuhao Wang</a>,</span>
                      <span class="author-block">
                        <a href="https://silongyong.github.io/" target="_blank">Silong Yong</a>,</span>
                          <span class="author-block">
                            <a href="https://simonstepputtis.com/" target="_blank">Simon Stepputtis</a>,</span>
                            <span class="author-block">
                              <a href="https://scholar.google.com/citations?user=VWv6a9kAAAAJ&hl=en" target="_blank">Katia Sycara</a>,</span>
                              <span class="author-block">
                                <a href="https://scholar.google.com.hk/citations?user=lBCCo0EAAAAJ&hl=zh-CN&oi=ao" target="_blank">Yaqi Xie</a></span>
                                <!-- <span class="author-block"> -->
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Robotics Institute, Carnegie Mellon University<br><b>WACV 2025</b></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">

                    <!-- PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/Sigma.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- Supplementary PDF link -->
                  <span class="link-block">
                    <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2404.04256" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/zifuwan/Sigma" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
              </span>
              <figure>
                <br>
                <img src="static/images/sigma.png" alt="fail" width="100%"">
                <figcaption class="content has-text-left"  style="word-break:normal"><b>Figure 1. Overall architecture of Sigma.</b> Our proposed method comprises a siamese mamba encoder, a fusion module, and a channel-aware mamba decoder. During the encoding phase, four Visual State Space Blocks (VSSB) with downsampling operations are sequentially cascaded to extract multi-level image features. Subsequently, features from each level, derived from the two branches, are processed through a fusion module. In the decoding phase, the fused features at each level are further enhanced by a Channel-Aware Visual State Space Block (CAVSSB) with an upsampling operation. Ultimately, the final feature is forwarded to a classifier to generate the prediction. More details can be found in the paper.
                </figure>

                <figure>
                  <br>
                  <img src="static/images/overall_flops.png" alt="fail" width="100%"">
                  <figcaption class="content has-text-left"  style="word-break:normal"><b>Figure 2. Efficiency analysis.</b> (a) Comparative analysis of complexity across different fusion methods utilizing Transformer and Mamba: Mamba-based fusion approaches significantly reduce complexity by an order of magnitude compared to their Transformer-based counterparts. (b) Computation and model size comparison of Sigma and other methods on MFNet dataset. The size of each circle indicates the model size (parameters).
                </figure>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper motivation -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract & Highlights</h2>
        <div class="content has-text-justified">
          <p>
            Multi-modal semantic segmentation significantly enhances AI agents' perception and scene understanding, especially under adverse conditions like low-light or overexposed environments. Leveraging additional modalities (X-modality) like thermal and depth alongside traditional RGB provides complementary information, enabling more robust and reliable prediction. In this work, we introduce Sigma, a Siamese Mamba network for multi-modal semantic segmentation utilizing the advanced Mamba. Unlike conventional methods that rely on CNNs, with their limited local receptive fields, or Vision Transformers (ViTs), which offer global receptive fields at the cost of quadratic complexity, our model achieves global receptive fields with linear complexity. By employing a Siamese encoder and innovating a Mamba-based fusion mechanism, we effectively select essential information from different modalities. A decoder is then developed to enhance the channel-wise modeling ability of the model. Our proposed method is rigorously evaluated on both RGB-Thermal and RGB-Depth semantic segmentation tasks, demonstrating its superiority and marking the first successful application of State Space Models (SSMs) in multi-modal perception tasks.
          </p>
          <p>
            <ul>
              <li>To our best knowledge, our work marks the first successful application of state space models, specifically Mamba, in multi-modal semantic segmentation.
              <li>We introduce a Mamba-based fusion mechanism alongside a channel-aware decoder, to efficiently extract information across different modalities and integrate them seamlessly.
              <li>Comprehensive evaluations in RGB-Thermal and RGB-Depth domains showcase our method's superior accuracy and efficiency, setting a new benchmark for future investigations into Mamba's potential in multi-modal learning.</li>
            </ul>
          </p>
        </div>
      </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results</h2>

        <div class="content has-text-left is-size-5">
        <strong>Quantitative Results on RGB-Thermal Datasets</strong>
        </div>
        <figure>
          <img src="static/images/quantitative_rgbt.png" alt="fail" width="100%"">
          <figcaption class="content has-text-left" style="word-break:normal"><b>Table 1. Quantitative comparisons for semantic segmentation of RGB-T images on MFNet and PST900 datasets.</b> The best and second best results in each block are highlighted in <b>bold</b> and <u>underline</u>, respectively.
        </figure>
        <br>
        <div class="content has-text-left is-size-5">
          <strong>Quantitative Results on RGB-Depth Datasets</strong>
        </div>
        <figure>
          <img src="static/images/quantitative_rgbd.png" alt="fail" width="70%">
          <figcaption class="content has-text-left" style="word-break:normal">
            <b>Table 2. Qualitative comparisons of RGB-D semantic segmentation on NYU Depth V2 and SUN RGB-D.</b> The best and second best results in each block are highlighted in <b>bold</b> and <u>underline</u>, respectively.
          </figure>
        <br>
        <div class="content has-text-left is-size-5">
          <strong>Qualitative Comparisons</strong>
        </div>
        <figure style="margin-bottom: 20px;">
          <img src="static/images/qualitative_mfnet.png" alt="fail" width="100%">
          <figcaption class="content has-text-left" style="word-break:normal">
            <b>Figure 3. Qualitative comparison on MFNet dataset.</b> More results can be found in the supplementary material.
        </figure>
        <figure style="margin-bottom: 20px;">
          <img src="static/images/qualitative_nyu.png" alt="fail" width="100%">
          <figcaption class="content has-text-left" style="word-break:normal">
            <b>Figure 4. Qualitative comparison on NYU Depth V2 dataset.</b> We use HHA images for better visualization of depth modality. More results can be found in the supplementary material.
        </figure>
        <figure>
          <img src="static/images/multi_vs_single.png" alt="fail" width="100%">
          <figcaption class="content has-text-left" style="word-break:normal">
            <b>Figure 5. Comparative analysis of semantic segmentation results: single-modal vs. multi-modal approach.</b>
        </figure>
      </div>
    </div>
  </div>
</section>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      
      <pre><code>@article{wan2024sigma,
  title={Sigma: Siamese mamba network for multi-modal semantic segmentation},
  author={Wan, Zifu and Wang, Yuhao and Yong, Silong and Zhang, Pingping and Stepputtis, Simon and Sycara, Katia and Xie, Yaqi},
  journal={arXiv preprint arXiv:2404.04256},
  year={2024}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
